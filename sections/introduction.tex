\section{Introduction}

High performance computing resources utilize batch systems to manage the user
workload. Job schedulers are designed to intelligently determine when jobs
should run, optimizing for various goals such as high utilization or minimal
queue wait. Resource managers typically accept job submissions and handle job
launch. Cray systems are uniquely different from typical clusters due to an
additional layer called Cray’s Application Level Placement Scheduler (ALPS).
ALPS manages binary transfer, job launch and monitoring, and error handling.
Batch systems require special support to integrate with ALPS using an XML
protocol called BASIL.

Previous versions of Adaptive Computing’s TORQUE and Moab batch suite
integrated with ALPS from within Moab, using PERL scripts to interface with
BASIL. This would occasionally lead to problems when all the components would
become unsynchronized. Additionally, TORQUE was unaware of the Cray compute
nodes. Version 4.1 of the TORQUE Resource Manager introduced new features that
allow it to directly integrate with ALPS using the BASIL protocol.

This paper describes early experiences with the newest versions of the TORQUE
resource manager. Early on, software bugs related to the newly-introduced
multithreading features prevented successful deployment of the new versions of
TORQUE. Through close collaboration with Adaptive Computing, the software
improved significantly to the point where it was acceptable for use on the
Titan and Gaea systems. Additionally, this paper describes production
experiences at Oak Ridge National Lab using the new TORQUE software versions
and describes future collaborative work to improve Cray-enabled TORQUE.
