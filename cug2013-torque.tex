\documentclass[10pt, conference, compsocconf]{IEEEtran}

% Various packages that might be useful
\usepackage[pdftex]{graphicx}
\usepackage{array}
%\usepackage[tight,footnotesize]{subfigure}
%\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{color}
\usepackage{underscore}


\begin{document}

\title{Production Experiences with the Cray-Enabled TORQUE Resource Manager}

\author{\IEEEauthorblockN{Matt Ezell and Don Maxwell}
\IEEEauthorblockA{High Performance Computing Operations\\
Oak Ridge National Laboratory\\
Oak Ridge, TN\\
\{ezellma,maxwellde\}@ornl.gov}
\and
\IEEEauthorblockN{David Beer}
\IEEEauthorblockA{Senior Software Engineer\\
Adaptive Computing\\
Provo, UT\\
dbeer@adaptivecomputing.com}
}

\maketitle


\begin{abstract}
High performance computing resources utilize batch systems to manage the user
workload. Cray systems are uniquely different from typical clusters due to
Cray’s Application Level Placement Scheduler (ALPS). ALPS manages binary
transfer, job launch and monitoring, and error handling. Batch systems require
special support to integrate with ALPS using an XML protocol called BASIL.

Previous versions of Adaptive Computing’s TORQUE and Moab batch suite integrated
with ALPS from within Moab, using PERL scripts to interface with BASIL. This
would occasionally lead to problems when all the components would become
unsynchronized. Version 4.1 of the TORQUE Resource Manager introduced new
features that allow it to directly integrate with ALPS using BASIL. This paper
describes production experiences at Oak Ridge National Lab using the new TORQUE
software versions.
\end{abstract}

\begin{IEEEkeywords}
TORQUE; Resource Manager; Adaptive Computing; Cray; ALPS; Moab; HPC; Titan; Gaea
\end{IEEEkeywords}


\input{sections/introduction}

\input{sections/alps}

\input{sections/prev-design}

\input{sections/new-design}
The new design greatly simplifies the implementation by reducing integration points and
coupling things where they naturally make sense. The minutiae of creating, confirming, and
releasing jobs doesn't need to be performed by a scheduler; moving this all to mom daemons
allows Moab to do what it does best – schedule. Mom daemons already manage everything that 
has to do with setting up jobs, starting them, and cleaning up after them once completed, 
so the ALPS interactions belong on the moms. Finally, having all of the integration a 
single daemon reduces integration points, making the code easier to understand and maintain.

\subsection{How It Works}
\begin{figure}
  \centering
  \includegraphics[width=5.0in]{figures/new_diagram.pdf}
  \caption{New Reporting Structure}\label{fig:reporting}
\end{figure}

All ALPS integration now takes place on the mom daemons. Two different kinds of mom daemons are
run for this setup: a reporter mom and one or more alps_login moms. The reporter mom's only function is to report
the inventory information to pbs_server, which then discovers all of the computes. 

The other mom daemon is the alps_login type, which manages job starts. The login mom creates and confirms
the reservations for the job before launching it. When the job is done, the mom releases its ALPS
reservation. The job start process is diagramed below in \ref{fig:starting}.

Moab no longer knows anything about ALPS. From Moab's perspective, it is scheduling a two different
partitions (clusters) - the login nodes in one partition and the compute nodes in the other. Moab is
only making scheduling decisions, and so it is now only given information relevant to scheduling. This is 
outlined in \ref{fig:reporting}. Moab maintains the ability to schedule login-only jobs for the 
purpose of compilation or other simple, short tasks. 

The pbs_server ties the two together. When a node status is given, the reporter mom's status is presented
as the status of all of the compute nodes, and the logins' statuses are given as any other pbs_mom's 
status appears. When Moab runs a job, it only selects which compute nodes that job will use, and pbs_server
uses a round robin method to select which login daemon should be responsible for starting that job. 

\begin{figure}
  \centering
  \includegraphics[width=3.0in]{figures/job_start.pdf}
  \caption{Job Starting}\label{fig:starting}
\end{figure}

\subsection{Advantages}
The new design simplifies configuration. No special binaries for Moab, TORQUE's server or mom
daemons are required, not even for the distinct types of mom daemons. Less configuration makes
for easier setup.

The whole model inherits superior support from Moab because from Moab's perspective, scheduling 
the Cray is the same as scheduling any other cluster, and it is no longer a one-off. Additionally,
the code path that was used for parsing output from the scripts in significantly slower than the 
code path for interfacing with TORQUE for two reasons: text parsing happens faster and the interaction
is now through an API instead of a fork/pipe model.

Moab and pbs_server can now be moved outside of the Cray, allowing a number of benefits. You can
install them on as powerful of nodes as you like, and aren't bound to whatever you have already.
If, after having the machine for a while, you decide you need to upgrade the server and scheduler
nodes then you can do this without upgrading the entire machine. This also gives you back whatever
resources on the Cray Moab and pbs_server were occupying. 

\section{Production Experiences}

\input{sections/gaea}

\input{sections/titan}

\input{sections/future-work}

\input{sections/conclusion}

\input{sections/acknowledgment}

% references section
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,cug2013-torque}

% that's all folks
\end{document}


